!!****if* source/physics/Hydro/HydroMain/Spark/Hydro
!! NOTICE
!!  Copyright 2022 UChicago Argonne, LLC and contributors
!!
!!  Licensed under the Apache License, Version 2.0 (the "License");
!!  you may not use this file except in compliance with the License.
!!
!!  Unless required by applicable law or agreed to in writing, software
!!  distributed under the License is distributed on an "AS IS" BASIS,
!!  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
!!  See the License for the specific language governing permissions and
!!  limitations under the License.
!!
!!
!! NAME
!!
!!  Hydro
!!
!!   For more details see the documentation of the NULL implementation
!!
!!***
!!Reorder(4): hy_fl[xyz], Uin

subroutine Hydro(timeEndAdv, dt, dtOld, sweepOrder)
  use Hydro_data, ONLY : hy_useHydro, hy_fluxCorrect, hy_fluxCorrectPerLevel, hy_starState, &
       hy_gcMask, hy_lChyp, hy_C_hyp, hy_globalComm,hy_geometry,hy_del,hy_tmpState, hy_tState,&
       hy_flx, hy_fly, hy_flz, hy_fluxBufX, hy_fluxBufY, hy_fluxBufZ,hy_tiny, hy_hybridRiemann,&
       hy_farea,hy_cvol,hy_xCenter,hy_xLeft,hy_xRight,hy_yCenter,hy_zCenter,hy_Vc, hy_useTelescoping
  use Hydro_inhost_data, ONLY : hy_pgrv,hy_pshck,hy_pflat,hy_flat3d,hy_pen
  use hy_rk_interface, ONLY : @M hy_rk_ff_name, @M hy_grav_name, @M hy_update_name, hy_rk_correctFluxes
  use Timers_interface, ONLY : Timers_start, Timers_stop
  use Grid_interface, ONLY : Grid_communicateFluxes, &
       Grid_fillGuardCells, Grid_getMaxRefinement, &
       Grid_correctFluxData_xtra, Grid_putFluxData_block, &
       Grid_getFluxCorrData_block,Grid_renormAbundance
  use Grid_interface, ONLY : Grid_getCellCoords, Grid_getCellFaceAreas, &
       Grid_getCellVolumes
  use hy_memInterface, ONLY : hy_memGetBlkPtr
  
  use Eos_interface, ONLY : Eos_wrapped
  use IO_interface, ONLY : IO_setScalar
  @M iter_use
  
  implicit none
  
#include "Simulation.h"
#include "constants.h"
#include "Eos.h"
#include "Spark.h"
  include "Flashx_mpi.h"
  
#define NRECON HY_NUM_VARS+NSPECIES+NMASS_SCALARS
  @M iter_declare(blkLimits, blkLimitsGC,grownLimits)

  real,    intent(in) :: timeEndAdv, dt, dtOld
  integer, intent(IN) :: sweepOrder
  integer :: n, error, maxLev=-1
  integer :: xLo,xHi,yLo,yHi,zLo,zHi
  integer, dimension(LOW:HIGH,MDIM) :: limits
  real :: hdt
  integer :: stage, last_stage
  real, dimension(3) :: coeffs, weights
  integer, dimension(3) :: limits_array
  real, dimension(3,3) :: coeff_array
    logical, dimension (3) :: addFlux_array
  integer :: i,j,k,v,maxcells,lev
  
    real, pointer :: Utemp(:,:,:,:)
  
  integer, dimension(MDIM) :: lo, hi, loGC, hiGC
  integer :: xLoGC,yLoGC,zLoGC,xHiGC,yHiGC,zHiGC
  integer :: pLo,pHi !low and high indices for pencil arrays

  if (.NOT. hy_useHydro) return
  call Timers_start("Hydro")
  
  @M hy_rk_getset
  @M saveGlobalState
  !-------------------------------------------------------------------!
  !***NO Flux correction    or   Flux correction but NOT per level****!
  !-------------------------------------------------------------------!
  if (.NOT.hy_fluxCorrectPerLevel) then
     ! Loop over blocks and compute Hydro update block-by-block
     if(hy_useTelescoping) then
        call Grid_fillGuardCells(CENTER,ALLDIR,doEos=.false.,maskSize=NUNK_VARS,mask=hy_gcMask)
        nullify(Uin)
        @M iter_all_begin(LEAF,.false.,blkLimits,blkLimitsGC)
             @M hy_rk_firstStage
             !Begin loop over stages
             do stage=1,last_stage
                @M hy_rk_oneStage
                @M hy_rk_lastStage                         
             enddo!stage loop
             !Store flux buffer in semipermanent flux storage (SPFS) 
             if (hy_fluxCorrect) call Grid_putFluxData_block(tileDesc,&
                  hy_fluxBufX,hy_fluxBufY,hy_fluxBufZ,blkLimits(LOW,:)) 
             call deallocate_scr()
        @M iter_end
     else
        do stage = 1,last_stage
           call Grid_fillGuardCells(CENTER,ALLDIR,doEos=.false.,maskSize=NUNK_VARS,mask=hy_gcMask)
           nullify(Uin)
           @M iter_all_begin(LEAF,.false.,blkLimits,blkLimitsGC)

           @M hy_rk_firstStage

           @M hy_rk_oneStage
             !Store flux buffer in semipermanent flux storage (SPFS) 
             if (hy_fluxCorrect) call Grid_putFluxData_block(tileDesc,&
                  hy_fluxBufX,hy_fluxBufY,hy_fluxBufZ,blkLimits(LOW,:)) 
             call deallocate_scr()
          @M iter_end
       end do
    end if
  else !flux correct per level
     print *, "Flux correct per level"
     !----------------------------------------!
     !*****Flux correction per level Occurs***!
     !----------------------------------------!
     if(hy_useTelescoping) then
        call Grid_fillGuardCells(CENTER,ALLDIR,doEos=.false.,maskSize=NUNK_VARS,mask=hy_gcMask)
        
        do lev=maxLev,1,-1
           
           !Once the finest level is completed, place averaged fine fluxes into
           !current coarse semipermanent flux storage (SPFS)
           if (level < maxLev) call Grid_communicateFluxes(ALLDIR,level)
           ! Loop over blocks and compute Hydro update block-by-block
           !~ For now tiling is disabled until we can confirm block registers are the same as tile registers
           nullify(Uin)
           @M iter_level_begin(LEAF,.FALSE.,lev,blkLimits,blkLimitsGC)
           @M hy_rk_firstStage
           !Loop stages
           do stage=1,last_stage
              @M hy_rk_oneStage
              @M hy_rk_lastStage              
           end do!stage loop
           !Put flux buffer information into SPFS
           if (level > 1) call Grid_putFluxData_block(tileDesc,hy_fluxBufX,hy_fluxBufY,hy_fluxBufZ,&
                blkLimits(LOW,:))
           call deallocate_scr()
           @M iter_end        
        enddo!loop over levels
     else
        do stage = 1,last_stage
           call Grid_fillGuardCells(CENTER,ALLDIR,doEos=.false.,maskSize=NUNK_VARS,mask=hy_gcMask)
           do lev=maxLev,1,-1
              
              !Once the finest level is completed, place averaged fine fluxes into
              !current coarse semipermanent flux storage (SPFS)
              if (level < maxLev) call Grid_communicateFluxes(ALLDIR,level)
              ! Loop over blocks and compute Hydro update block-by-block
              !~ For now tiling is disabled until we can confirm block registers are the same as tile registers
              nullify(Uin)
              @M iter_level_begin(LEAF,.FALSE.,lev,blkLimits,blkLimitsGC)
                 @M hy_rk_firstStage

                 @M hy_rk_oneStage
                 !Put flux buffer information into SPFS
                 if (level > 1) call Grid_putFluxData_block(tileDesc,hy_fluxBufX,hy_fluxBufY,hy_fluxBufZ,&
                      blkLimits(LOW,:))
                 call deallocate_scr()
             @M iter_end
          end do
       end do
    end if
 end if
 
 if (hy_fluxCorrect .and. (.NOT.hy_fluxCorrectPerLevel)) then
     !Communicate the fine fluxes
     call Grid_communicateFluxes(ALLDIR,UNSPEC_LEVEL)
     
     !        call Timers_start("flux correct")
     !        ! Loop over blocks and correct block-edge solution
     
        nullify(Uin)
        @M iter_all_begin(LEAF,.false.,blkLimits,blkLimitsGC) 
           hy_del=deltas
           @M hy_tmp_ind
           !           !Get 'Flux hy_del' on coarse side of fine coarse boundaries; 
           !           !all other values are 0.
           call allocate_fxscr(blkLimits,blkLimitsGC)
           @M hy_geomSp_alloc
           call Grid_getFluxCorrData_block(tileDesc,hy_fluxBufX,hy_fluxBufY,hy_fluxBufZ,&
                blkLimits(LOW,:))
           call hy_rk_correctFluxes(Uin,blkLimits,blklimitsGC,level,hy_del, dt)
           @M hy_geomSp_dealloc
           call deallocate_fxscr()
        @M iter_end
     end if !Flux correction
  

! Reset local maximum hyperbolic speed. This will be updated in Hydro_computeDt.
  hy_lChyp = TINY(1.0)
  
  call Timers_stop("Hydro")
  
contains
  
#include "@M hy_funcs_name.F90"
  
  
  subroutine check_if_on_GPU()
!$  use omp_lib, ONLY : omp_is_initial_device
    use Driver_interface, ONLY : Driver_abort
    implicit none
    logical :: onCPU
    
    onCPU = .TRUE.
    
    !$omp target map(tofrom:onCPU)
    !$  onCPU = omp_is_initial_device()
    !$omp end target
    
    if (onCPU) then
       print *, "---------------- Running on CPU --------------------------------"
       ! call Driver_abort("Unable to run on GPU")
    else
       print *, "---------------- Running on GPU --------------------------------"
    end if
    
  end subroutine check_if_on_GPU
  
end subroutine Hydro
